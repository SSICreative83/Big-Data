{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark and Hadoop\n",
    "\n",
    "In this notebook we look at real data while using our small hadoop cluster. For programming we will use parallel dataframes. This will give us experience with real data and big dataframes with Spark.\n",
    "\n",
    "We look at [the New York City Taxi Cab dataset](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml). This includes every ride made in the city of New York in the year 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a subset \n",
    "\n",
    "Normally we would read and load this data into memory as a Pandas dataframe.  However in this case that would be unwise because this data is too large to fit comfortably in RAM (do not try, you will kill your session).\n",
    "\n",
    "The data can stay in the hdfs filesystem. The following function read the first row of the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.9.6.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[16]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f987532a208>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[16]') \\\n",
    "        .config('spark.hadoop.parquet.enable.summary-metadata', 'true') \\\n",
    "        .getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    \"hdfs://localhost:54310//user/pnavaro/2016_Yellow_Taxi_Trip_Data.csv\", \n",
    "                    header=\"true\",inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: int, trip_distance: double, pickup_longitude: double, pickup_latitude: double, RatecodeID: int, store_and_fwd_flag: string, dropoff_longitude: double, dropoff_latitude: double, payment_type: int, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, PULocationID: int, DOLocationID: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('hdfs://localhost:54310/user/pnavaro/nyc-taxi/2016.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Parquet format](https://github.com/apache/parquet-format) is a common binary data store, used particularly in the Hadoop/big-data sphere. It provides several advantages relevant to big-data processing:\n",
    "\n",
    "- columnar storage, only read the data of interest\n",
    "- efficient binary packing\n",
    "- choice of compression algorithms and encoding\n",
    "- split data into files, allowing for parallel processing\n",
    "- range of logical types\n",
    "- statistics stored in metadata allow for skipping unneeded chunks\n",
    "- data partitioning using the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs3 import HDFileSystem\n",
    "hdfs = HDFileSystem(host='localhost', port=54310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lorem import paragraph\n",
    "with hdfs.open('/user/pnavaro/samples.txt','wb') as f:\n",
    "    f.write(paragraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/user/pnavaro/1990.csv',\n",
       " '/user/pnavaro/1991.csv',\n",
       " '/user/pnavaro/1992.csv',\n",
       " '/user/pnavaro/1993.csv',\n",
       " '/user/pnavaro/1994.csv',\n",
       " '/user/pnavaro/1995.csv',\n",
       " '/user/pnavaro/1996.csv',\n",
       " '/user/pnavaro/1997.csv',\n",
       " '/user/pnavaro/1998.csv',\n",
       " '/user/pnavaro/1999.csv',\n",
       " '/user/pnavaro/2016_Yellow_Taxi_Trip_Data.csv',\n",
       " '/user/pnavaro/copied-file.txt',\n",
       " '/user/pnavaro/nyc-taxi',\n",
       " '/user/pnavaro/nycflights.parquet',\n",
       " '/user/pnavaro/remote-file.txt',\n",
       " '/user/pnavaro/samples.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs.ls('/user/pnavaro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lorem import text\n",
    "with open('local-file.txt','w') as f:\n",
    "    f.write(text())\n",
    "hdfs.put('local-file.txt', '/user/pnavaro/remote-file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/user/pnavaro/1990.csv',\n",
       " '/user/pnavaro/1991.csv',\n",
       " '/user/pnavaro/1992.csv',\n",
       " '/user/pnavaro/1993.csv',\n",
       " '/user/pnavaro/1994.csv',\n",
       " '/user/pnavaro/1995.csv',\n",
       " '/user/pnavaro/1996.csv',\n",
       " '/user/pnavaro/1997.csv',\n",
       " '/user/pnavaro/1998.csv',\n",
       " '/user/pnavaro/1999.csv',\n",
       " '/user/pnavaro/2016_Yellow_Taxi_Trip_Data.csv',\n",
       " '/user/pnavaro/copied-file.txt',\n",
       " '/user/pnavaro/nyc-taxi',\n",
       " '/user/pnavaro/nycflights.parquet',\n",
       " '/user/pnavaro/remote-file.txt',\n",
       " '/user/pnavaro/samples.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs.ls('/user/pnavaro/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs.mv('/user/pnavaro/remote-file.txt', '/user/pnavaro/copied-file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/user/pnavaro/1990.csv',\n",
       " '/user/pnavaro/1991.csv',\n",
       " '/user/pnavaro/1992.csv',\n",
       " '/user/pnavaro/1993.csv',\n",
       " '/user/pnavaro/1994.csv',\n",
       " '/user/pnavaro/1995.csv',\n",
       " '/user/pnavaro/1996.csv',\n",
       " '/user/pnavaro/1997.csv',\n",
       " '/user/pnavaro/1998.csv',\n",
       " '/user/pnavaro/1999.csv',\n",
       " '/user/pnavaro/2016_Yellow_Taxi_Trip_Data.csv',\n",
       " '/user/pnavaro/copied-file.txt',\n",
       " '/user/pnavaro/nyc-taxi',\n",
       " '/user/pnavaro/nycflights.parquet',\n",
       " '/user/pnavaro/remote-file.txt',\n",
       " '/user/pnavaro/samples.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs.ls('/user/pnavaro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fastparquet](http://fastparquet.readthedocs.io/en/latest/) provides a performant library to read and write Parquet files from Python, without any need for a Python-Java bridge. This will make the Parquet format an ideal storage mechanism for Python-based big data workflows.\n",
    "\n",
    "The tabular nature of Parquet is a good fit for the Pandas data-frame objects, and we exclusively deal with data-frame<->Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Apache Arrow](https://arrow.apache.org/docs/python/)\n",
    "\n",
    "Arrow is a columnar in-memory analytics layer designed to accelerate big data. It houses a set of canonical in-memory representations of flat and hierarchical data along with multiple language-bindings for structure manipulation.\n",
    "\n",
    "https://arrow.apache.org/docs/python/parquet.html\n",
    "\n",
    "The Apache Parquet project provides a standardized open-source columnar storage format for use in data analysis systems. It was created originally for use in Apache Hadoop with systems like Apache Drill, Apache Hive, Apache Impala (incubating), and Apache Spark adopting it as a shared standard for high performance data IO.\n",
    "\n",
    "Apache Arrow is an ideal in-memory transport layer for data that is being read or written with Parquet files. [PyArrow](https://arrow.apache.org/docs/python/) includes Python bindings to read and write Parquet files with pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Parquet File: {'name': '/user/pnavaro/nyc-taxi/2016.parquet/_metadata', 'columns': ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'RatecodeID', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'PULocationID', 'DOLocationID'], 'partitions': [], 'rows': 131165043}>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastparquet as fp\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "hdfs = pa.hdfs.connect('localhost', 54310, 'pnavaro', driver='libhdfs3')\n",
    "\n",
    "pf = fp.ParquetFile('/user/pnavaro/nyc-taxi/2016.parquet', open_with=hdfs.open)\n",
    "pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.master('spark://schedulers:7077').getOrCreate()\n",
    "#spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['tpep_pickup_datetime', 'passenger_count', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'tip_amount', 'total_amount']\n",
    "\n",
    "df = (spark.read.parquet('hdfs://localhost:54310/user/pnavaro/nyc-taxi/2016.parquet')\n",
    "           .select(*columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sum the total number of passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(passenger_count)=217355302)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg({'passenger_count': 'sum'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(passenger_count)=1.6571130312517794)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg({'passenger_count': 'avg'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(passenger_count=1, count(1)=92987719),\n",
       " Row(passenger_count=6, count(1)=4234423),\n",
       " Row(passenger_count=3, count(1)=5456807),\n",
       " Row(passenger_count=5, count(1)=6773026),\n",
       " Row(passenger_count=9, count(1)=261),\n",
       " Row(passenger_count=4, count(1)=2660369),\n",
       " Row(passenger_count=8, count(1)=316),\n",
       " Row(passenger_count=7, count(1)=361),\n",
       " Row(passenger_count=2, count(1)=19038307),\n",
       " Row(passenger_count=0, count(1)=13454)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('passenger_count').agg({'*': 'count'}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the `filter`, `groupBy`, `agg` operations to find out how well New Yorkers tip based on the number of passengers in the cab.\n",
    " 1. Remove rides with zero fare\n",
    " 2. Add a new column tip_fraction that is equal to the ratio of the tip to the fare\n",
    " 3. Group by the passenger_count column and take the mean of the tip_fraction column.\n",
    " \n",
    "You may want to refer to these resources to help you with the Spark DataFrame API\n",
    "- https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "- https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "And refer to the Spark UI for feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to remove rows\n",
    "\n",
    "In Spark you can filter rows by a boolean expression like the following:\n",
    "```python\n",
    "df.filter(df.name == 'Alice')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to make new columns\n",
    "\n",
    "In Pandas you can create a new column using Python's setitem syntax like the following:\n",
    "\n",
    "```python\n",
    "df = df.withColumn('z', df.x + df.y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do groupby-aggregations\n",
    "\n",
    "In Pandas you can do a groupby-aggregation by using the `groupby` method, followed by a column name an aggregation method like the following:\n",
    "\n",
    "```python\n",
    "df.groupBy(df.name).agg({'column-name': 'avg'})\n",
    "```\n",
    "\n",
    "When you want to collect the result of your computation, finish with the `.collect()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. How well do New Yorkers tip as a function of the hour of day and the day of the week?\n",
    "2. Investiate the `payment_type` column.  See how well each of the payment types correlate with the `tip_fraction`.  Did you find anything interesting?  Any guesses on what the different payment types might be?  If you're interested you may be able to find more information on the [NYC TLC's website](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml)\n",
    "3. How quickly can you get the data for a particular day of the year?  How about for a particular hour of that day?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
