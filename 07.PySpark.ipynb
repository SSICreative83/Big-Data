{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PySpark\n",
    "-------\n",
    "\n",
    "Spark can manage \"big data\" collections with a small set of high-level primitives like `map`, `filter`, `groupby`, and `join`.  With these common patterns we can often handle computations that are more complex than map, but are still structured.\n",
    "\n",
    "PySpark uses Py4J that enables Python programs to dynamically access Java objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![PySpark Internals](http://i.imgur.com/YlI8AqEl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Apache Spark is a fast and general-purpose cluster computing system. \n",
    "- It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. \n",
    "- It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
    "sc = SparkContext('local[2]') # Create a local spark cluster with 2 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# If tou get an error run this cell with the command below commented out\n",
    "# and fix the path to python in the cell above\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have a spark context sc to use with a tiny local spark cluster with 2 nodes (will work just fine on a multicore machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[2] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc) # it is like a Pool Processor executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(5))  # create collection\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()  # Gather results back to local process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x ** 2) # Square each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x ** 2).collect() # Square each element and collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map-Reduce operation \n",
    "from operator import add\n",
    "rdd.map(lambda x: x ** 2).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the even elements\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (3, 0),\n",
       " (4, 0),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (3, 2),\n",
       " (4, 2),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (4, 3),\n",
       " (4, 4)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cartesian product of each pair of elements in two sequences \n",
    "# (or the same sequence in this case)\n",
    "rdd.cartesian(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (16, 0),\n",
       " (16, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (16, 2),\n",
       " (16, 3),\n",
       " (16, 4)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain operations to construct more complex computations\n",
    "(rdd.map(lambda x: x ** 2)\n",
    "    .cartesian(rdd)\n",
    "    .filter(lambda tup: tup[0] % 2 == 0)\n",
    "    .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Stop the local spark cluster\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pi computation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.138772\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PythonPi\").getOrCreate()\n",
    "\n",
    "partitions = 4\n",
    "n = 1000000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = spark.sparkContext.parallelize(range(1, n+1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 7.1\n",
    "\n",
    "Using the same method than the PI computation example, compute the integral\n",
    "$$\n",
    "I = \\int_0^1 \\exp(-x^2) dx\n",
    "$$\n",
    "You can check your result with sympy or scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√π⋅erf(x)\n",
      "─────────\n",
      "    2    \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$0.746824132812427$$"
      ],
      "text/plain": [
       "0.746824132812427"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sym  # sympy computes the exact solution using symbolic calculus.\n",
    "sym.init_printing(use_latex='mathjax')\n",
    "x = sym.Symbol('x')\n",
    "I = sym.integrate(sym.exp(-x*x),x)\n",
    "sym.pprint(I)\n",
    "I.subs(x,1).evalf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left ( 0.7468241328124271, \\quad 8.291413475940725e-15\\right )$$"
      ],
      "text/plain": [
       "(0.7468241328124271, 8.291413475940725e-15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy and scipy evaluates solution using numeric computation. It uses discrete values\n",
    "# of the function\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "quad(lambda x: np.exp(-x*x), 0, 1)\n",
    "# note: the solution returned is complex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercice 7.2\n",
    "\n",
    "We again start with the following sequential code\n",
    "\n",
    "```python\n",
    "series = {}\n",
    "for fn in filenames:   # Simple map over filenames\n",
    "    series[fn] = pd.read_hdf(fn)['x']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for a in filenames:    # Doubly nested loop over the same collection\n",
    "    for b in filenames:  \n",
    "        if a != b:     # Filter out bad elements\n",
    "            results[a, b] = series[a].corr(series[b])  # Apply function\n",
    "\n",
    "((a, b), corr) = max(results.items(), key=lambda kv: kv[1])  # Reduction\n",
    "```\n",
    "\n",
    "Parallelize pairwise correlations with PySpark\n",
    "\n",
    "To make this a bit easier we're just going to compute the maximum correlation and not try to keep track of the stocks that yielded this maximal result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spark methods\n",
    "\n",
    "We can construct most of the above computation with the following Spark methods:\n",
    "\n",
    "*  `collection.map(function)`: apply function to each element in collection\n",
    "*  `collection.cartesian(collection)`: Create new collection with every pair of inputs\n",
    "*  `collection.filter(predicate)`: Keep only elements of colleciton that match the predicate function\n",
    "*  `collection.max()`: Compute maximum element\n",
    "\n",
    "We use these briefly in isolated exercises and then combine them to rewrite the previous computation from the `submit` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished : hal.h5\n",
      "Finished : hp.h5\n",
      "Finished : hpq.h5\n",
      "Finished : ibm.h5\n",
      "Finished : jbl.h5\n",
      "Finished : jpm.h5\n",
      "Finished : luv.h5\n",
      "Finished : pcg.h5\n",
      "Finished : usb.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/json/hal.h5',\n",
       " 'data/json/hp.h5',\n",
       " 'data/json/hpq.h5',\n",
       " 'data/json/ibm.h5',\n",
       " 'data/json/jbl.h5',\n",
       " 'data/json/jpm.h5',\n",
       " 'data/json/luv.h5',\n",
       " 'data/json/pcg.h5',\n",
       " 'data/json/usb.h5']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import ujson as json # or json\n",
    "\n",
    "def convert_to_json(d):\n",
    "    \"\"\" Convert all csv files of directory d into json format \"\"\"\n",
    "    filenames = sorted(glob(os.path.join(d, '*')))[-365:]\n",
    "    outfn = d.replace('minute', 'json') + '.json'\n",
    "    if os.path.exists(outfn):\n",
    "        return\n",
    "    with open(outfn, 'w') as f:\n",
    "        for fn in filenames:\n",
    "            df = pd.read_csv(fn)\n",
    "            for rec in df.to_dict(orient='records'):\n",
    "                json.dump(rec, f)\n",
    "                f.write('\\n')\n",
    "    print(\"Finished JSON: %s\" % d.split(os.path.sep)[-1])\n",
    "\n",
    "here = os.getcwd()\n",
    "js = os.path.join(here, 'data', 'json')\n",
    "if not os.path.exists(js):\n",
    "    os.mkdir(js)\n",
    "\n",
    "directories = sorted(glob(os.path.join(here, 'data', 'minute', '*')))\n",
    "for d in directories:\n",
    "    convert_to_json(d)\n",
    "\n",
    "filenames = sorted(glob(os.path.join('data', 'json', '*.json')))\n",
    "\n",
    "for fn in filenames:\n",
    "    with open(fn) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    out_filename = fn[:-5] + '.h5'\n",
    "    df.to_hdf(out_filename, '/data')\n",
    "    print(\"Finished : %s\" % out_filename.split(os.path.sep)[-1])\n",
    "\n",
    "filenames = sorted(glob(os.path.join('data', 'json', '*.h5')))  # ../data/json/*.json\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 847 ms, sys: 193 ms, total: 1.04 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### Sequential Code\n",
    "\n",
    "series = []\n",
    "for fn in filenames:   # Simple map over filenames\n",
    "    series.append(pd.read_hdf(fn)['close'])\n",
    "\n",
    "results = []\n",
    "\n",
    "for a in series:    # Doubly nested loop over the same collection\n",
    "    for b in series:  \n",
    "        if not (a == b).all():     # Filter out comparisons of the same series \n",
    "            results.append(a.corr(b))  # Apply function\n",
    "\n",
    "result = max(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corr' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc = SparkContext('local[2]')\n",
    "rdd = sc.parallelize(filenames)\n",
    "\n",
    "# TODO\n",
    "\n",
    "result = corr\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$0.946000933948$$"
      ],
      "text/plain": [
       "0.946000933948"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Computation time is slower because there is a lot of setup, workers creation, there is a lot of communications the correlation function is too small"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
