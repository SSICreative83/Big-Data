{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data\n",
    "\n",
    "- Data sets that are so large or complex that traditional data processing application software is inadequate to deal with them. \n",
    "- Data analysis requires massively parallel software running on several servers.\n",
    "- **Volume, Variety, Velocity, Variability and Veracity** describe Big Data properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "![Hadoop Logo](http://hadoop.apache.org/images/hadoop-logo.jpg)\n",
    "\n",
    "- Framework for running applications on large cluster. \n",
    "- The Hadoop framework transparently provides applications both reliability and data motion. \n",
    "- Hadoop implements the computational paradigm named **Map/Reduce**, where the application is divided into many small fragments of work, each of which may be executed or re-executed on any node in the cluster. \n",
    "- It provides a distributed file system (HDFS) that stores data on the compute nodes, providing very high aggregate bandwidth across the cluster.\n",
    "- Both MapReduce and the **Hadoop Distributed File System** are designed so that node failures are automatically handled by the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HDFS\n",
    "* It is a distributed file systems.\n",
    "* HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware.\n",
    "* HDFS is suitable for applications that have large data sets. \n",
    "* HDFS provides interfaces to move applications closer to where the data is located. The computation is much more efficient when the size of the data set is huge. \n",
    "* HDFS consists of a single NameNode with a number of DataNodes which manage storage. \n",
    "* HDFS exposes a file system namespace and allows user data to be stored in files. \n",
    "    1. A file is split by the NameNode into blocks stored in DataNodes. \n",
    "    2. The **NameNode** executes operations like opening, closing, and renaming files and directories.\n",
    "    3. The **Secondary NameNode** stores information from **NameNode**. \n",
    "    4. The **DataNodes** manage perform block creation, deletion, and replication upon instruction from the NameNode.\n",
    "    5. The placement of replicas is optimized for data reliability, availability, and network bandwidth utilization.\n",
    "    6. User data never flows through the NameNode.\n",
    "* Files in HDFS are write-once and have strictly one writer at any time.\n",
    "* The DataNode has no knowledge about HDFS files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accessibility\n",
    "\n",
    "All [HDFS commands](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)  are invoked by the bin/hdfs Java script:\n",
    "```shell\n",
    "hdfs [SHELL_OPTIONS] COMMAND [GENERIC_OPTIONS] [COMMAND_OPTIONS]\n",
    "```\n",
    "## Manage files and directories\n",
    "```shell\n",
    "hdfs dfs -ls -h -R # Recursively list subdirectories with human-readable file sizes.\n",
    "hdfs dfs -cp  # Copy files from source to destination\n",
    "hdfs dfs -mv  # Move files from source to destination\n",
    "hdfs dfs -mkdir /foodir # Create a directory named /foodir\t\n",
    "hdfs dfs -rmr /foodir   # Remove a directory named /foodir\t\n",
    "hdfs dfs -cat /foodir/myfile.txt #View the contents of a file named /foodir/myfile.txt\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer between nodes\n",
    "\n",
    "## put\n",
    "```shell\n",
    "hdfs fs -put [-f] [-p] [-l] [-d] [ - | <localsrc1> .. ]. <dst>\n",
    "```\n",
    "Copy single src, or multiple srcs from local file system to the destination file system. \n",
    "\n",
    "Options:\n",
    "\n",
    "    -p : Preserves rights and modification times.\n",
    "    -f : Overwrites the destination if it already exists.\n",
    "\n",
    "```shell\n",
    "hdfs fs -put localfile /user/hadoop/hadoopfile\n",
    "hdfs fs -put -f localfile1 localfile2 /user/hadoop/hadoopdir\n",
    "```\n",
    "Similar to the fs -put command\n",
    "- `moveFromLocal` : to delete the source localsrc after copy.\n",
    "- `copyFromLocal` : source is restricted to a local file\n",
    "- `copyToLocal` : destination is restricted to a local file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![hdfs blocks](http://saphanatutorial.com/wp-content/uploads/2014/06/Hadoop-Course-4.jpg)\n",
    "\n",
    "The Name Node is not in the data path. The Name Node only provides the map of where data is and where data should go in the cluster (file system metadata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NameNode Web Interface (HDFS layer) \n",
    "\n",
    "http://localhost:50070/\n",
    "        \n",
    "The name node web UI shows you a cluster summary including information about total/remaining capacity, live and dead nodes. Additionally, it allows you to browse the HDFS namespace and view the contents of its files in the web browser. It also gives access to the local machine’s Hadoop log files.\n",
    "\n",
    "# Secondary Namenode Information.\n",
    "\n",
    "http://localhost:50090/\n",
    "\n",
    "# Datanode Information.\n",
    "\n",
    "http://localhost:50075/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- Make your HDFS home directory required to execute MapReduce jobs:\n",
    "```bash\n",
    "hdfs dfs -mkdir -p /user/${USER}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/11/01 14:31:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/11/01 14:31:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "drwxr-xr-x   - navaro supergroup          0 2017-10-31 16:06 books\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /user/${USER}\n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Log on to the cluster and type the following commands: \n",
    "```bash\n",
    "hdfs dfs -ls\n",
    "hdfs dfs -ls /\n",
    "hdfs dfs -mkdir books\n",
    "```\n",
    "- Create a local file user.txt containing your name and the date:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "echo \"Pierre Navaro\" > user.txt\n",
    "echo `date` >> user.txt \n",
    "cat user.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre Navaro\r\n",
      "Mer 1 nov 2017 14:32:02 CET\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"Pierre Navaro\" > user.txt\n",
    "!echo `date` >> user.txt \n",
    "!cat user.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Copy it on  HDFS :\n",
    "```bash\n",
    "hdfs dfs -put user.txt\n",
    "```\n",
    "\n",
    "Check with:\n",
    "```bash\n",
    "hdfs dfs -ls -R \n",
    "hdfs dfs -cat user.txt \n",
    "hdfs dfs -tail user.txt \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/11/01 14:32:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "17/11/01 14:32:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "drwxr-xr-x   - navaro supergroup          0 2017-10-31 16:06 books\n",
      "-rw-r--r--   1 navaro supergroup     674570 2017-10-31 16:06 books/20417.txt\n",
      "-rw-r--r--   1 navaro supergroup    1580890 2017-10-31 16:06 books/4300-0.txt\n",
      "-rw-r--r--   1 navaro supergroup    1428841 2017-10-31 16:06 books/5000-8.txt\n",
      "-rw-r--r--   1 navaro supergroup         42 2017-11-01 14:32 user.txt\n",
      "17/11/01 14:32:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Pierre Navaro\n",
      "Mer 1 nov 2017 14:32:02 CET\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put user.txt\n",
    "!hdfs dfs -ls -R\n",
    "!hdfs dfs -cat user.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remove the file:\n",
    "```bash\n",
    "hdfs dfs -rm user.txt\n",
    "```\n",
    "\n",
    "Put it again on HDFS and move to books directory:\n",
    "```bash\n",
    "hdfs dfs -copyFromLocal user.txt\n",
    "hdfs dfs -mv user.txt books/user.txt\n",
    "hdfs dfs -ls -R -h\n",
    "```\n",
    "\n",
    "Copy user.txt to hello.txt and remove it.\n",
    "```bash\n",
    "hdfs dfs -cp books/user.txt books/hello.txt\n",
    "hdfs dfs -count -h /user/$USER\n",
    "hdfs dfs -rm books/user.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on practice:\n",
    "\n",
    "1. Create a directory `files` in HDFS.\n",
    "2. List the contents of a directory /.\n",
    "3. Upload the file today.txt in HDFS.\n",
    "```bash\n",
    "date > today.txt\n",
    "whoami >> today.txt\n",
    "```\n",
    "4. Display contents of file `today.txt`\n",
    "5. Copy `today.txt` file from source to `files` directory.\n",
    "6. Copy file `jps.txt` from/To Local file system to HDFS\n",
    "```bash\n",
    "jps > jps.txt\n",
    "```\n",
    "7. Move file `jps.txt` from source to `files`.\n",
    "8. Remove file `today.txt` from home directory in HDFS.\n",
    "9. Display last few lines of `jps.txt`.\n",
    "10. Display the help of `du` command and show the total amount of space in a human-readable fashion used by your home hdfs directory.\n",
    "12. Display the help of `df` command and show the total amount of space available in the filesystem in a human-readable fashion.\n",
    "13. With `chmod` change the rights of `today.txt` file. I has to be readable and writeable only by you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# YARN\n",
    "\n",
    "*YARN takes care of resource management and job scheduling/monitoring.*\n",
    "\n",
    "- The **ResourceManager** is the ultimate authority that arbitrates resources among all the applications in the system. It has two components: **Scheduler** and **ApplicationsManager**.\n",
    "- The **NodeManager** is the per-machine framework agent who is responsible for **Containers**, monitoring their resource usage (cpu, memory, disk, network) and reporting the same to the **ResourceManager/Scheduler**.\n",
    "\n",
    "The per-application **ApplicationMaster** negotiates resources from the ResourceManager and working with the NodeManager(s) to execute and monitor the tasks.\n",
    "\n",
    "- The **Scheduler** is responsible for allocating resources to the applications.\n",
    "\n",
    "- The **ApplicationsManager** is responsible for accepting job-submissions, tracking their status and monitoring for progress.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Yarn in Hadoop documentation](http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Yarn Web Interface\n",
    "\n",
    "The JobTracker web UI provides information about general job statistics of the Hadoop cluster, running/completed/failed jobs and a job history log file. It also gives access to the ‘‘local machine’s’’ Hadoop log files (the machine on which the web UI is running on).\n",
    "\n",
    " - All Applications http://localhost:8088/.\n",
    " - Node Specific Info: http://localhost:8042/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Run MapReduce example job.\n",
    "\n",
    "```bash\n",
    "mkdir input\n",
    "hdfs dfs -cp /home/user/navaro/books/* input\n",
    "hadoop jar \\\n",
    "/usr/local/hadoop-2.7.4/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.4.jar grep input output 'dfs[a-z.]+'\n",
    "hdfs dfs -ls output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WordCount Example \n",
    "\n",
    "The [Worcount example](https://wiki.apache.org/hadoop/WordCount) is implemented in Java and it is the example of [MapReduce Tutorial](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)\n",
    "\n",
    "The program reads text files and counts how often words occur. The input is text files and the output is text files, each line of which contains a word and the count of how often it occured.\n",
    "\n",
    "Download three ebooks from Project Gutenberg as input data.\n",
    "- The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson\n",
    "- The Notebooks of Leonardo Da Vinci — Complete by da Vinci Leonardo\n",
    "- Ulysses by James Joyce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```bash\n",
    "wget ... -e use_proxy=yes -e http_proxy=127.0.0.1:8080\n",
    "```\n",
    "\n",
    "```bash\n",
    "mkdir -p books\n",
    "wget -q -O books/20417.txt  http://www.gutenberg.org/ebooks/20417.txt.utf-8\n",
    "wget -q -O books/5000-8.txt http://www.gutenberg.org/files/5000/5000-8.txt\n",
    "wget -q -O books/4300-0.txt http://www.gutenberg.org/files/4300/4300-0.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "- Copy all necessary files in HDFS system.\n",
    "\n",
    "- Run the Java example using the command\n",
    "\n",
    "```bash\n",
    "hadoop jar <somewhere_in_hadoop_install_directory>hadoop*examples*.jar wordcount <input_directory> <output_directory>\n",
    "```\n",
    "\n",
    "hint: Use the unix command find.\n",
    "\n",
    "The result begin by:\n",
    "```text\n",
    "\"(Lo)cra\"\t1\n",
    "\"1490\t1\n",
    "\"1498,\"\t1\n",
    "\"35\"\t1\n",
    "\"40,\"\t1\n",
    "\"A\t2\n",
    "\"AS-IS\".\t1\n",
    "\"A_\t1\n",
    "\"Absoluti\t1\n",
    "\"Aesopi\"\t1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Deploying the MapReduce code on Hadoop\n",
    "\n",
    "This Python must use the [Hadoop Streaming API](http://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) to pass data between our Map and Reduce code via Python’s sys.stdin (standard input) and sys.stdout (standard output). \n",
    "\n",
    "Download some books\n",
    "* [The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson](http://www.gutenberg.org/ebooks/20417.txt.utf-8)\n",
    "* [Ulysses by James Joyce](http://www.gutenberg.org/files/4300/4300-0.txt)\n",
    "* [The Art of War by 6th cent. B.C. Sunzi](http://www.gutenberg.org/ebooks/132.txt.utf-8)\n",
    "* [The Adventures of Sherlock Holmes by Sir Arthur Conan Doyle](http://www.gutenberg.org/ebooks/1661.txt.utf-8)\n",
    "* [The Devil’s Dictionary by Ambrose Bierce](http://www.gutenberg.org/ebooks/972.txt.utf-8)\n",
    "* [Encyclopaedia Britannica, 11th Edition, Volume 4, Part 3](http://www.gutenberg.org/ebooks/19699.txt.utf-8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Map \n",
    "\n",
    "The following Python code read data from sys.stdin, split it into words and output a list of lines mapping words to their (intermediate) counts to sys.stdout. For every word it outputs <word> 1 tuples immediately. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hadoop/mapper.py\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hadoop/mapper.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-40c5b9d75ea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hadoop/mapper.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"#!/usr/bin/env python\\nfrom __future__ import print_function\\nimport sys, string\\n\\n# input comes from standard input\\nfor line in sys.stdin:\\n    # remove leading and trailing whitespace\\n    line = line.strip()\\n    # strip punctuation\\n    line = line.translate(None,string.punctuation)    \\n\\n    # split the line into words\\n    words = line.split()\\n    # increase counters\\n    for word in words:\\n        # write the results to standard output;\\n        # what we output here will be the input for the\\n        # Reduce step, i.e. the input for reducer.py\\n        #\\n        # tab-delimited; the trivial word count is 1\\n        print ('%s\\\\t%s' % (word, 1))\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2129\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2131\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0margs_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic_params\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-106>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hadoop/mapper.py'"
     ]
    }
   ],
   "source": [
    "%%file hadoop/mapper.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import sys, string\n",
    "\n",
    "# input comes from standard input\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # strip punctuation\n",
    "    line = line.translate(None,string.punctuation)    \n",
    "\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to standard output;\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print ('%s\\t%s' % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chmod: hadoop/mapper.py: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!chmod +x hadoop/mapper.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce \n",
    "\n",
    "The following code reads the results of mapper.py and sum the occurrences of each word to a final count, and then output its results to sys.stdout.\n",
    "Remember that Hadoop sorts map output so it is easier to count words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file hadoop/reducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input lines\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to sys.stdout\n",
    "            print ('{}\\t{}'.format(current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print ('{}\\t{}'.format(current_word, current_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat hadoop/sample.txt | ./hadoop/mapper.py | sort | ./hadoop/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution on Hadoop cluster\n",
    "\n",
    "* Copy books to HDFS\n",
    "* Run the WordCount MapReduce\n",
    "\n",
    "Makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file hadoop/Makefile\n",
    "HADOOP_TOOLS=/usr/local/Cellar/hadoop/2.8.1/libexec/share/hadoop/tools/lib/\n",
    "HDFS_DIR=/user/${USER}\n",
    "\n",
    "BOOKS = books/20417.txt \\\n",
    "books/4300-0.txt books/132.txt books/1661.txt books/972.txt books/19699.txt \n",
    "\n",
    "download:\n",
    "\tmkdir -p books\n",
    "\twget -q -O books/20417.txt http://www.gutenberg.org/ebooks/20417.txt.utf-8\n",
    "\twget -q -O books/4300-0.txt http://www.gutenberg.org/files/4300/4300-0.txt\n",
    "\twget -q -O books/132.txt http://www.gutenberg.org/ebooks/132.txt.utf-8\n",
    "\twget -q -O books/1661.txt http://www.gutenberg.org/ebooks/1661.txt.utf-8\n",
    "\twget -q -O books/972.txt http://www.gutenberg.org/ebooks/972.txt.utf-8\n",
    "\twget -q -O books/19699.txt http://www.gutenberg.org/ebooks/19699.txt.utf-8\n",
    "\n",
    "copy_to_hdfs: ${BOOKS}\n",
    "\thdfs dfs -put books books\n",
    "\t\n",
    "run_with_hadoop: \n",
    "\thadoop jar ${HADOOP_TOOLS}/hadoop-streaming-2.8.1.jar \\\n",
    "    -file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
    "    -file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
    "    -input ${HDFS_DIR}/books/* -output ${HDFS_DIR}/output-hadoop\n",
    "\n",
    "run_with_yarn: \n",
    "\tyarn jar ${HADOOP_TOOLS}/hadoop-streaming-2.8.1.jar \\\n",
    "\t-file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
    "\t-file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
    "\t-input ${HDFS_DIR}/books/* -output ${HDFS_DIR}/output-yarn\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
