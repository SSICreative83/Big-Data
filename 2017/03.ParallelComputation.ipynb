{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallel Computation\n",
    "\n",
    "## Parallel computers\n",
    "- Multiprocessor/multicore: several processors work on data stored in shared memory\n",
    "- Cluster: several processor/memory units work together by exchanging data over a network\n",
    "- Co-processor: a general-purpose processor delegates specific tasks to a special-purpose processor (GPU, Xeon Phi,...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel Programming\n",
    "- Decomposition of the complete task into independent subtasks and the data flow between them.\n",
    "- Distribution of the subtasks over the processors minimizing the total execution time.\n",
    "- For clusters: distribution of the data over the nodes minimizing the communication time.\n",
    "- For multiprocessors: optimization of the memory access patterns minimizing waiting times.\n",
    "- Synchronization of the individual processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import sleep\n",
    "def f(x):\n",
    "    sleep(1)\n",
    "    return x*x\n",
    "L = list(range(8))\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.74 ms, sys: 2.81 ms, total: 4.55 ms\n",
      "Wall time: 8.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sum([f(x) for x in L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.07 ms, sys: 2.61 ms, total: 4.68 ms\n",
      "Wall time: 8.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sum(map(f,L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiprocessing \n",
    "\n",
    "<p>\n",
    "<font color=red> This first part with multiprocessing does not work\n",
    "    on Windows </font>\n",
    "    </p>\n",
    "\n",
    "\n",
    "The multiprocessing allows the programmer to fully leverage multiple processors.\n",
    "- The Pool object parallelizes the execution of a function across multiple input values.\n",
    "- The if __name__ == '__main__' part is necessary.\n",
    "- The multiprocessing Pool class provides a map function. Partition and distribute input to a user-specified function in pool of worker processes is automatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "CPU times: user 20.7 ms, sys: 20.5 ms, total: 41.2 ms\n",
      "Wall time: 2.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from multiprocessing import Pool\n",
    "\n",
    "if __name__ == '__main__': # Executed only on main process.\n",
    "    with Pool() as p:\n",
    "        print(sum(p.map(f, L))) # Apply f on L sequence and sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Pool() launches one slave process per physical processor on the computer. \n",
    "- pool.map(...) divides the input list into chunks and puts the tasks (function + chunk) on a queue.\n",
    "- Each slave process takes a task (function + a chunk of data), runs map(function, chunk), and puts the result on a result list.\n",
    "- pool.map on the master process waits until all tasks are handled and returns the concatenation of the result lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "- Use `paragraph` function module from `lorem` to create a text\n",
    "- Create a list of words from it\n",
    "- Use `map` function from `multiprocessing.Pool` to compute each word length\n",
    "- Compare time with sequential version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 7 6 4 7 3 3 11 10 4 7 10 5 4 5 4 11 4 6 4 4 5 8 4 10 6 11 11 3 4 8 10 5\n",
      "CPU times: user 2.04 ms, sys: 940 Âµs, total: 2.98 ms\n",
      "Wall time: 2.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lorem import paragraph\n",
    "\n",
    "words_list = paragraph().lower().replace('.','').split()\n",
    "print(*map(len,words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 7 6 4 7 3 3 11 10 4 7 10 5 4 5 4 11 4 6 4 4 5 8 4 10 6 11 11 3 4 8 10 5\n",
      "CPU times: user 13.7 ms, sys: 19.6 ms, total: 33.3 ms\n",
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from multiprocessing import Pool\n",
    "\n",
    "if __name__ == '__main__': # Executed only on main process.\n",
    "    with Pool() as p:\n",
    "        results = p.map(len, words_list)# Apply f on L sequence and sum\n",
    "\n",
    "print(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thread and Process: Differences\n",
    "\n",
    "- A Process is an instance of a running program. \n",
    "- Process may contain one or more threads, but a thread cannot contain a process.\n",
    "- Process has a self-contained execution environment. It has its own memory space. \n",
    "- Application running on your computer may be a set of cooperating processes.\n",
    "\n",
    "- A Thread is made of and exist within a Process; every process has at least one. \n",
    "- Multiple threads in a process share resources, which helps in efficient communication between threads.\n",
    "- Threads can be concurrent on a multi-core system, with every core executing the separate threads simultaneously.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Global Interpreter Lock (GIL)\n",
    "\n",
    "- The Python interpreter is not thread safe.\n",
    "- A few critical internal data structures may only be accessed by one thread at a time. Access to them is protected by the GIL.\n",
    "- Attempts at removing the GIL from Python have failed until now. The main difficulty is maintaining the C API for extension modules.\n",
    "- Multiprocessing avoids the GIL by having separate processes which each have an independent copy of the interpreter data structures.\n",
    "- The price to pay: serialization of tasks, arguments, and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Futures\n",
    "\n",
    "The `concurrent.futures` module provides a high-level interface for asynchronously executing callables.\n",
    "\n",
    "\n",
    "\n",
    "The asynchronous execution can be performed with:\n",
    "- **threads**, using ThreadPoolExecutor, \n",
    "- separate **processes**, using ProcessPoolExecutor. \n",
    "Both implement the same interface, which is defined by the abstract Executor class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`concurrent.futures` does not work on windows. Windows users must install \n",
    "[loky](https://github.com/tomMoral/loky)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install loky  # Windows users will need to install loky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "CPU times: user 17.2 ms, sys: 19.6 ms, total: 36.8 ms\n",
      "Wall time: 2.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "# from loky import ProcessPoolExecutor  # for Windows users\n",
    "e = ProcessPoolExecutor()\n",
    "\n",
    "results = sum(e.map(f, L))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "CPU times: user 5.29 ms, sys: 3.99 ms, total: 9.28 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "e = ThreadPoolExecutor()\n",
    "\n",
    "results = sum(e.map(f, L))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 3.2\n",
    "\n",
    "- Use `ProcessPoolExecutor` to compute each word length.\n",
    "- Use `map` to apply `len` function on each word of a text created with lorem module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 10 11 4 7 6 3 8 3 7 3 10 5 5 10 7 10 6 5 6 5 9 6 5 2 8 11 4 8 3 6 6 8 3 6 3 4 7 7 7 7 6 6 3 3 5 4 6 12 6 11 8 7 3 3 7 8 5 7 4 6 5 6 6 2 8 3 11 5 11 7 10 5 8 7 2 3 4 6 8 7 11 7 4 2 10 4 8 5 10 7 10 4 6 7 5 3 7 3 3 3 4 11 11 6 3 4 3 10 3 8 7 10 7 6 6 7 6 7 7 10 3 7 4 8 5 7 4 7 7 7 7 10 5 3 10 4 5 3 7 11 4 6 3 8 7 7 3 8 5 3 8 4 2 3 4 6 8 4 3 3 3 7 5 5 6 7 7 3 11 3 7 6 5 5 7 3 4 5 7\n"
     ]
    }
   ],
   "source": [
    "from lorem import text\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "#from loky import ProcessPoolExecutor  # for Windows\n",
    "\n",
    "texte = text()\n",
    "\n",
    "e = ProcessPoolExecutor(4)\n",
    "word_lengths = e.map(len, texte.split())\n",
    "print(*word_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 3.3\n",
    "\n",
    "Same as exercise 3.2 but use `ThreadPoolExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 8 8 8 6 3 6 3 5 7 2 6 8 7 3 5 10 7 4 6 3 5 5 4 6 7 7 6 5 11 5 11 10 3 6 7 2 7 8 6 5 5 10 5 8 6 3 10 6 10 2 6 7 7 11 7 7 7 7 3 7 4 5 12 10 7 3 8 8 7 2 2 11 8 3 10 5 6 3 5 2 5 3 7 6 10 7 5 7 5 4 10 3 7 6 5 5 10 7 10 7 4 7 10 10 10 5 2 6 6 7 2 2 4 4 11 7 7 3 7 7 5 5 7 7 10 6 6 4 8 5 7 3 5 8 10 2 3 5 5 6 6 5 10 3 5 8 5 7 5 11 6 10 8 5 6 8 6 3 4 5 6 6 11 5 3 5 3 7 12 3 10 7 3 2 6 5 8 5 6 9 4 4 2 7 6 4 3 7 3 5 5 5 4 7 5 3 3 6 4 10 6 4 3 3 4 4 11 7\n"
     ]
    }
   ],
   "source": [
    "from lorem import text\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "texte = text()\n",
    "\n",
    "e = ProcessPoolExecutor(4)\n",
    "word_lengths = e.map(len, texte.split())\n",
    "print(*word_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Map\n",
    "\n",
    "This words version contains some improvements and print out the \n",
    "process number where the function is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainProcess reading sample.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('adipisci', 1),\n",
       " ('adipisci', 1),\n",
       " ('adipisci', 1),\n",
       " ('adipisci', 1),\n",
       " ('adipisci', 1),\n",
       " ('adipisci', 1),\n",
       " ('adipisci', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('magnam', 1),\n",
       " ('magnam', 1),\n",
       " ('magnam', 1),\n",
       " ('magnam', 1),\n",
       " ('magnam', 1),\n",
       " ('magnam', 1),\n",
       " ('magnam', 1),\n",
       " ('magnam', 1),\n",
       " ('magnam', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sit', 1),\n",
       " ('sit', 1),\n",
       " ('sit', 1),\n",
       " ('sit', 1),\n",
       " ('sit', 1),\n",
       " ('sit', 1),\n",
       " ('tempora', 1),\n",
       " ('tempora', 1),\n",
       " ('tempora', 1),\n",
       " ('tempora', 1),\n",
       " ('tempora', 1),\n",
       " ('tempora', 1),\n",
       " ('tempora', 1),\n",
       " ('tempora', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('velit', 1),\n",
       " ('velit', 1),\n",
       " ('velit', 1),\n",
       " ('velit', 1),\n",
       " ('velit', 1),\n",
       " ('velit', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import multiprocessing as mp  # Windows users should comment this line\n",
    "def words_mp(file):\n",
    "    \"\"\"\n",
    "    Check if file is utf8\n",
    "    Read a text file and return a sorted list of (word, 1) values.\n",
    "    \"\"\"\n",
    "    # Windows users should comment this line below\n",
    "    print(mp.current_process().name, 'reading', file)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    output = []\n",
    "    try:\n",
    "        with open(file) as f:\n",
    "            for line in f:   \n",
    "                line = line.strip()\n",
    "                line = line.translate(translator)\n",
    "                for word in line.split():\n",
    "                    if word.isalpha():\n",
    "                        word = word.lower()\n",
    "                        output.append((word, 1))\n",
    "                        \n",
    "    except UnicodeDecodeError as err:\n",
    "        print(\"Some error occurred decoding file %s: %s\" % (file, err))\n",
    "                \n",
    "    output.sort()\n",
    "    return output\n",
    "\n",
    "from lorem import text\n",
    "\n",
    "with open('sample.txt','w') as f:\n",
    "    f.write(text())\n",
    "    \n",
    "words_mp('sample.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Partition\n",
    "Before parallel reduce operation, data must be aligned in a container. We create a function named `partition_mp` that stores the key/value pairs from `words_mp` into a `defaultdict` from collections module. Ouput is:\n",
    "[('word1', [1, 1]), ('word2', [1]), ('word3', [1, 1, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def partition_mp(mapped_values):\n",
    "    \"\"\"\n",
    "        Organize the mapped values by their key.\n",
    "        Returns an unsorted sequence of tuples \n",
    "        with a key and a sequence of values.\n",
    "    \"\"\"\n",
    "    partitioned_data = collections.defaultdict(list)\n",
    "    for key, value in mapped_values:\n",
    "        partitioned_data[key].append(value)\n",
    "    return partitioned_data.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainProcess reading sample.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_items([('adipisci', [1, 1, 1, 1, 1, 1, 1]), ('aliquam', [1, 1, 1, 1, 1, 1, 1, 1, 1]), ('amet', [1, 1, 1, 1, 1, 1, 1, 1, 1]), ('consectetur', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('dolor', [1, 1, 1, 1, 1, 1, 1, 1, 1]), ('dolore', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('dolorem', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('eius', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('est', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('etincidunt', [1, 1, 1, 1, 1, 1, 1, 1]), ('ipsum', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('labore', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('magnam', [1, 1, 1, 1, 1, 1, 1, 1, 1]), ('modi', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('neque', [1, 1, 1, 1, 1, 1, 1]), ('non', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('numquam', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('porro', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('quaerat', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('quiquia', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('quisquam', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), ('sed', [1, 1, 1, 1, 1, 1, 1, 1, 1]), ('sit', [1, 1, 1, 1, 1, 1]), ('tempora', [1, 1, 1, 1, 1, 1, 1, 1]), ('ut', [1, 1, 1, 1, 1, 1, 1, 1]), ('velit', [1, 1, 1, 1, 1, 1]), ('voluptatem', [1, 1, 1, 1, 1, 1, 1, 1, 1])])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_mp(words_mp('sample.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mp(item):\n",
    "    \"\"\"Convert the partitioned data for a word to a\n",
    "    tuple containing the word and the number of occurances.\n",
    "    \"\"\"\n",
    "    word, occurances = item\n",
    "    return (word, len(occurances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainProcess reading sample.txt\n",
      "('adipisci', 7)\n",
      "('aliquam', 9)\n",
      "('amet', 9)\n",
      "('consectetur', 10)\n",
      "('dolor', 9)\n",
      "('dolore', 17)\n",
      "('dolorem', 13)\n",
      "('eius', 14)\n",
      "('est', 12)\n",
      "('etincidunt', 8)\n",
      "('ipsum', 12)\n",
      "('labore', 11)\n",
      "('magnam', 9)\n",
      "('modi', 16)\n",
      "('neque', 7)\n",
      "('non', 11)\n",
      "('numquam', 10)\n",
      "('porro', 12)\n",
      "('quaerat', 13)\n",
      "('quiquia', 16)\n",
      "('quisquam', 11)\n",
      "('sed', 9)\n",
      "('sit', 6)\n",
      "('tempora', 8)\n",
      "('ut', 8)\n",
      "('velit', 6)\n",
      "('voluptatem', 9)\n"
     ]
    }
   ],
   "source": [
    "for occurences in partition_mp(words_mp('sample.txt')):\n",
    "    print(reduce_mp(occurences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainProcess reading sample06.txt\n",
      "MainProcess reading sample07.txt\n",
      "MainProcess reading sample05.txt\n",
      "MainProcess reading sample04.txt\n",
      "MainProcess reading sample00.txt\n",
      "MainProcess reading sample01.txt\n",
      "MainProcess reading sample03.txt\n",
      "MainProcess reading sample02.txt\n",
      "CPU times: user 7.68 ms, sys: 3.25 ms, total: 10.9 ms\n",
      "Wall time: 9.05 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools   \n",
    "import glob\n",
    "\n",
    "# Sequential version\n",
    "\n",
    "filenames = glob.glob(\"sample0*.txt\")\n",
    "mapped_values = map(words_mp, filenames)\n",
    "partionned_data = partition_mp(itertools.chain(*mapped_values))\n",
    "results = map(reduce_mp,partionned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('adipisci', 64) ('aliquam', 69) ('amet', 68) ('consectetur', 66) ('dolor', 71) ('dolore', 69) ('dolorem', 69) ('eius', 59) ('est', 63) ('etincidunt', 66) ('ipsum', 71) ('labore', 61) ('magnam', 59) ('modi', 59) ('neque', 71) ('non', 66) ('numquam', 70) ('porro', 86) ('quaerat', 60) ('quiquia', 58) ('quisquam', 72) ('sed', 70) ('sit', 84) ('tempora', 66) ('ut', 59) ('velit', 59) ('voluptatem', 73)\n"
     ]
    }
   ],
   "source": [
    "print(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 3.4\n",
    "\n",
    "Write a parallel program that uses the three functions above using `multiprocessing`. It reads all the \"sample\\*.txt\" files. Some hints:\n",
    "- Map and reduce steps are parallel.\n",
    "- See how `itertools.chain(*mapped_values)` is used in notebook exercise 01.6.\n",
    "- Compare time between the notebook 01 version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForkPoolWorker-22 reading sample07.txt\n",
      "ForkPoolWorker-21 reading sample06.txt\n",
      "ForkPoolWorker-23 reading sample05.txt\n",
      "ForkPoolWorker-22 reading sample01.txt\n",
      "ForkPoolWorker-24 reading sample04.txt\n",
      "ForkPoolWorker-21 reading sample00.txt\n",
      "ForkPoolWorker-23 reading sample03.txt\n",
      "ForkPoolWorker-22 reading sample02.txt\n",
      "[('porro', 86), ('sit', 84), ('voluptatem', 73), ('quisquam', 72), ('dolor', 71), ('ipsum', 71), ('neque', 71), ('numquam', 70), ('sed', 70), ('aliquam', 69), ('dolore', 69), ('dolorem', 69), ('amet', 68), ('consectetur', 66), ('etincidunt', 66), ('non', 66), ('tempora', 66), ('adipisci', 64), ('est', 63), ('labore', 61), ('quaerat', 60), ('eius', 59), ('magnam', 59), ('modi', 59), ('ut', 59), ('velit', 59), ('quiquia', 58)]\n",
      "CPU times: user 19.4 ms, sys: 24.5 ms, total: 43.9 ms\n",
      "Wall time: 134 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools   \n",
    "import glob\n",
    "import operator\n",
    "from multiprocessing import Pool\n",
    "\n",
    "filenames = glob.glob(\"sample0*.txt\")\n",
    "\n",
    "# Parallel version\n",
    "if __name__ == '__main__':\n",
    "    with Pool() as p:\n",
    "        mapped_values = p.map(words_mp, filenames)\n",
    "        partionned_data = partition_mp(itertools.chain(*mapped_values))\n",
    "        results = p.map(reduce_mp,partionned_data)\n",
    "print(sorted(results, key=operator.itemgetter(1), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 3.5\n",
    "\n",
    "- Replace `multiprocessing` by `concurrent.futures` functions.\n",
    "- Try  `ProcessPoolExecutor` and `ThreadPoolExecutor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lorem import text\n",
    "for i in range(8): # Create files sample0-7.txt\n",
    "    with open(\"sample{0:02d}.txt\".format(i), \"w\") as f:\n",
    "        f.write(text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainProcess reading sample06.txt\n",
      "MainProcess reading sample07.txt\n",
      "MainProcess reading sample05.txt\n",
      "MainProcess reading sample04.txt\n",
      "MainProcess reading sample00.txt\n",
      "MainProcess reading sample01.txt\n",
      "MainProcess reading sample03.txt\n",
      "MainProcess reading sample02.txt\n",
      "[('etincidunt', 75), ('consectetur', 67), ('tempora', 66), ('dolore', 65), ('porro', 63), ('quaerat', 62), ('dolorem', 61), ('eius', 60), ('labore', 60), ('neque', 60), ('non', 59), ('quiquia', 59), ('modi', 58), ('numquam', 57), ('magnam', 56), ('quisquam', 56), ('sit', 56), ('est', 55), ('ut', 55), ('aliquam', 54), ('adipisci', 52), ('amet', 51), ('voluptatem', 51), ('velit', 49), ('sed', 48), ('dolor', 47), ('ipsum', 47)]\n",
      "CPU times: user 18.6 ms, sys: 9.4 ms, total: 28 ms\n",
      "Wall time: 25.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools   \n",
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Parallel version\n",
    "e = ThreadPoolExecutor()\n",
    "filenames = glob.glob(\"sample0*.txt\")\n",
    "mapped_values = e.map(words_mp, filenames)\n",
    "partionned_data = partition_mp(itertools.chain(*mapped_values))\n",
    "results = e.map(reduce_mp,partionned_data)\n",
    "print(sorted(results, key=operator.itemgetter(1), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-26 reading sample07.txt\n",
      "Process-25 reading sample06.txt\n",
      "Process-27 reading sample05.txt\n",
      "Process-28 reading sample04.txt\n",
      "Process-25 reading sample03.txt\n",
      "Process-26 reading sample01.txt\n",
      "Process-27 reading sample00.txt\n",
      "Process-28 reading sample02.txt\n",
      "[('etincidunt', 75), ('consectetur', 67), ('tempora', 66), ('dolore', 65), ('porro', 63), ('quaerat', 62), ('dolorem', 61), ('eius', 60), ('labore', 60), ('neque', 60), ('non', 59), ('quiquia', 59), ('modi', 58), ('numquam', 57), ('magnam', 56), ('quisquam', 56), ('sit', 56), ('est', 55), ('ut', 55), ('aliquam', 54), ('adipisci', 52), ('amet', 51), ('voluptatem', 51), ('velit', 49), ('sed', 48), ('dolor', 47), ('ipsum', 47)]\n",
      "CPU times: user 35.2 ms, sys: 30.1 ms, total: 65.3 ms\n",
      "Wall time: 76 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools   \n",
    "import glob\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Parallel version\n",
    "e = ProcessPoolExecutor()\n",
    "filenames = glob.glob(\"sample0*.txt\")\n",
    "mapped_values = e.map(words_mp, filenames)\n",
    "partionned_data = partition_mp(itertools.chain(*mapped_values))\n",
    "results = e.map(reduce_mp,partionned_data)\n",
    "print(sorted(results, key=operator.itemgetter(1), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can use for your multi-processing computations both `multiprocessing.Pool` and  `concurrent.futures` object, which behaves more or less identically.\n",
    "\n",
    "However, today most library designers are coordinating around the  second interface, so it's wise to move over.\n",
    "\n",
    "`concurrent.futures.ProcessPoolExecutor` is suitable for simple parallelism across many files and you gain some speed boost. \n",
    "Describing each task as a function call helps use tools like map for parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Increase volume of data\n",
    "\n",
    "### Getting the data\n",
    "\n",
    "[The Latin Library](http://www.thelatinlibrary.com/) contains a huge collection of freely accessible Latin texts. We get links on the Latin Library's homepage ignoring some links that are not associated with a particular author.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting credentials.py\n"
     ]
    }
   ],
   "source": [
    "%%file credentials.py\n",
    "login = \"your sesame login\"\n",
    "password = \"your sesame password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import *\n",
    "from credentials import *\n",
    "\n",
    "\n",
    "#Uncomment lines below to enable proxy\n",
    "#proxy_url = login+':'+password+'@192.168.192.17:8080'\n",
    "#proxy = ProxyHandler({'http': 'http://'+proxy_url, 'https': 'https://'+proxy_url })\n",
    "#auth = HTTPBasicAuthHandler()\n",
    "#opener = build_opener(proxy, auth, HTTPHandler)\n",
    "#install_opener(opener)\n",
    "\n",
    "base_url = \"http://www.thelatinlibrary.com/\"\n",
    "home_content = urlopen(base_url)\n",
    "\n",
    "soup = BeautifulSoup(home_content, \"lxml\")\n",
    "author_page_links = soup.find_all(\"a\")\n",
    "author_pages = [ap[\"href\"] for i, ap in enumerate(author_page_links) if i < 49]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of all links pointing to Latin texts. The Latin Library uses a special format which makes it easy to find the corresponding links: All of these links contain the name of the text author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_content = list()\n",
    "for ap in author_pages:\n",
    "    ap_content.append(urlopen(base_url + ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"ammianus/14.shtml\">Liber XIV</a>, <a href=\"ammianus/15.shtml\">Liber XV</a>, <a href=\"ammianus/16.shtml\">Liber XVI</a>, <a href=\"ammianus/17.shtml\">Liber XVII</a>, <a href=\"ammianus/18.shtml\">Liber XVIII</a>]\n"
     ]
    }
   ],
   "source": [
    "book_links = list()\n",
    "for path, content in zip(author_pages, ap_content):\n",
    "    author_name = path.split(\".\")[0]\n",
    "    ap_soup = BeautifulSoup(content, \"lxml\")\n",
    "    book_links += ([link for link in ap_soup.find_all(\"a\", {\"href\": True}) if author_name in link[\"href\"]])\n",
    "\n",
    "print(book_links[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting content 10 of 10\r"
     ]
    }
   ],
   "source": [
    "texts = list()\n",
    "num_pages = 10\n",
    "\n",
    "for i, bl in enumerate(book_links[:num_pages]):\n",
    "    print(\"Getting content \" + str(i + 1) + \" of \" + str(num_pages), end=\"\\r\", flush=True)\n",
    "    try:\n",
    "        content = urlopen(base_url + bl[\"href\"]).read() \n",
    "        texts.append(content)\n",
    "    except HTTPError as err:\n",
    "        print(\"Unable to retrieve \" + bl[\"href\"] + \".\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the text at periods to convert it into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post emensos insuperabilis expeditionis eventus languentibus partium animis quas periculorum varietas fregerat et laborum nondum tubarum cessante clangore vel milite locato per stationes hibernas fortunae saevientis procellae tempestates alias rebus infudere communibus per multa illa et dira facinora caesaris galli qui ex squalore imo miseriarum in aetatis adultae primitiis ad principale culmen insperato saltu provectus ultra terminos potestatis delatae procurrens asperitate nimia cuncta foedabat\n",
      "CPU times: user 1.28 s, sys: 26.2 ms, total: 1.31 s\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = list()\n",
    "\n",
    "for text in texts:\n",
    "    print(\"Document \" + str(i + 1) + \" of \" + str(len(texts)), end=\"\\r\", flush=True)\n",
    "    textSoup = BeautifulSoup(text, \"lxml\")\n",
    "    paragraphs = textSoup.find_all(\"p\", attrs={\"class\":None})\n",
    "    prepared = (\"\".join([p.text.strip().lower() for p in paragraphs[1:-1]]))\n",
    "    for t in prepared.split(\".\"):\n",
    "        part = \"\".join([c for c in t if c.isalpha() or c.isspace()])\n",
    "        sentences.append(part.strip())\n",
    "\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.6\n",
    "\n",
    "Parallelize these last process using `concurrent.future`.\n",
    "Do not try to print out too much text, the notebook will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('sentences.txt', 'w') as outfile:\n",
    "    json.dump(sentences, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post emensos insuperabilis expeditionis eventus languentibus partium animis quas periculorum varietas fregerat et laborum nondum tubarum cessante clangore vel milite locato per stationes hibernas fortunae saevientis procellae tempestates alias rebus infudere communibus per multa illa et dira facinora caesaris galli qui ex squalore imo miseriarum in aetatis adultae primitiis ad principale culmen insperato saltu provectus ultra terminos potestatis delatae procurrens asperitate nimia cuncta foedabat\n",
      "CPU times: user 15.5 ms, sys: 22.9 ms, total: 38.4 ms\n",
      "Wall time: 120 ms\n",
      "Document 10 of 10\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import itertools\n",
    "\n",
    "def convert_to_sentences(text):\n",
    "    sentences = list()\n",
    "    print(\"Document \" + str(i + 1) + \" of \" + str(len(texts)), end=\"\\r\", flush=True)\n",
    "    textSoup = BeautifulSoup(text, \"lxml\")\n",
    "    paragraphs = textSoup.find_all(\"p\", attrs={\"class\":None})\n",
    "    prepared = (\"\".join([p.text.strip().lower() for p in paragraphs[1:-1]]))\n",
    "    for t in prepared.split(\".\"):\n",
    "        part = \"\".join([c for c in t if c.isalpha() or c.isspace()])\n",
    "        sentences.append(part.strip())\n",
    "    return sentences\n",
    "        \n",
    "e = ProcessPoolExecutor()\n",
    "result = e.map(convert_to_sentences, texts)\n",
    "\n",
    "print(next(result)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Using Conditional Random Fields and Python for Latin word segmentation](https://medium.com/@felixmohr/using-python-and-conditional-random-fields-for-latin-word-segmentation-416ca7a9e513)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
